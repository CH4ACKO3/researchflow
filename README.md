# ResearchFlow

> **Note**: This README is generated by AI. Please refer to the specific code for the implementation.

Streamline independent research projects with this toolkit for training process scheduling, model, data and results storage and management.

## Table of Contents

- [Overview](#overview)
- [Core Components](#core-components)
  - [MetadataStorage](#metadatastorage)
  - [Async GPU Scheduler](#async-gpu-scheduler)
- [Installation](#installation)
- [Testing](#testing)
- [License](#license)

## Overview

ResearchFlow provides two powerful tools to streamline machine learning research workflows:

1. **Metadata Storage**: A robust JSON-based index file storage system with flexible query and retrieval functions, multi-process safe, always able to locate your model and result files based on a small number of training parameters.
2. **Asynchronous GPU Scheduler**: An asynchronous GPU task scheduler that takes input in the most basic way: reading (and feedback) from a pre-generated command-line process task table and performing efficient task distribution among multiple GPUs.

All of the above are single-file implementations!
## Core Components

### Metadata Storage

A thread-safe, JSON-based indexed file storage system designed for managing experimental data, model checkpoints, and results with rich metadata.

#### Key Features

- **UUID-Based File Management**: Automatic UUID generation for unique file identification
- **Metadata Querying**: Powerful query system supporting both exact and partial metadata matching
- **Thread-Safe Operations**: Directory-level locking with nested locking support for concurrent access
- **CRUD Operations**: Complete Create, Read, Update, Delete functionality
- **Atomic Operations**: Atomic file writes with automatic backup and recovery
- **Orphaned File Cleanup**: Automatic detection and cleanup of files not referenced in index
- **Nested Structures**: Support for nested metadata and attachment hierarchies
- **Flexible Attachments**: Manage both individual files and entire directory structures
- **Index Persistence**: Automatic index persistence across storage instances with corruption recovery

#### Usage Example

```python
from pipelines.utils.storage import MetadataStorage

# Initialize storage
storage = MetadataStorage("experiment_data")

# Create entry with metadata
uuid = storage.create_entry(
    metadata={"model": "bert-base", "dataset": "squad", "epoch": 10},
    extra_info={"accuracy": 0.92, "loss": 0.15},
    attachments={"checkpoint": "models/checkpoint.pt"}
)

# Query by metadata (partial match)
entries, uuids = storage.read_entries(
    metadata_query={"model": "bert-base"}
)

# Update entry
storage.update_entry(uuid, metadata={"epoch": 20})

# Delete entry and associated files
storage.delete_entries(uuid_query=uuid)

# Cleanup orphaned files
storage.cleanup_orphaned_files()
```

---

### Async GPU Scheduler

An intelligent asynchronous task scheduler designed for efficient GPU resource management in multi-GPU training environments.

#### Key Features

- **Multi-GPU Support**: Automatically distributes tasks across available GPUs
- **Resource Monitoring**: 
  - Real-time GPU memory and utilization tracking
  - System memory monitoring with automatic throttling
  - 60-second rolling window for utilization averaging
- **Dynamic GPU Management**: Control GPU availability via `available_gpus.txt` file at runtime
- **Smart Scheduling**:
  - Configurable memory and utilization thresholds
  - Grace period to prevent task thrashing
  - Automatic task retry on GPU unavailability
  - System memory-based task admission control
- **Task State Management**: Comprehensive task lifecycle tracking
  - `waiting`: Queued for execution
  - `running`: Currently executing
  - `finished`: Successfully completed
  - `terminated`: Failed or killed
- **File-Based Task Definition**: Simple text file interface for task management
- **Automatic Archiving**: Completed job files moved to processed directory
- **Live Monitoring**: Real-time console display of GPU status and task progress
- **Comprehensive Logging**: 
  - Rotating daily logs with 7-day retention
  - Task start/completion timestamps
  - Detailed execution history

#### Usage

1. **Create a task file** (`jobs/my_tasks.txt`):

```
uv run train.py --model bert --dataset squad
uv run train.py --model roberta --dataset mnli
uv run train.py --model gpt2 --dataset wikitext
```

2. **Specify available GPUs** (`available_gpus.txt`):

```
0 1 2 3
```

3. **Start the scheduler**:

```bash
uv run scripts/async_train.py --max-memory 80 --max-util 80 --grace-period 180
```

**Command-line Arguments**:
- `--max-memory`: Maximum GPU memory usage percentage (default: 80.0)
- `--max-util`: Maximum GPU utilization percentage (default: 80.0)
- `--grace-period`: Seconds to wait before starting new task after last task completion (default: 180)

4. **Monitor progress**: Task status is updated in real-time in the job files:

```
# uv run train.py --model bert --dataset squad      # finished
! uv run train.py --model roberta --dataset mnli    # running
uv run train.py --model gpt2 --dataset wikitext     # waiting
```

5. **Dynamic GPU control**: Modify `available_gpus.txt` at runtime to add/remove GPUs

6. **View logs**: Check `logs/history/history.log` for detailed execution history

#### Task File Format

- **No prefix**: Task is waiting
- **`!` prefix**: Task is currently running
- **`#` prefix**: Task finished successfully
- **`?` prefix**: Task terminated with error

#### Architecture

- **TaskIO**: Asynchronous file monitoring and task queue management
- **ProcessorWorker**: Per-GPU worker managing task execution and resource monitoring
- **GPU Monitor**: Dynamic GPU availability tracking
- **Console Printer**: Real-time status display

---

## Installation

### Requirements

- Python >= 3.12
- CUDA-capable GPU(s) for async scheduler
- NVIDIA Management Library (NVML)

### Setup

#### Option 1: For New Projects (Clone from main branch)

1. **Clone the repository**:

```bash
git clone https://github.com/CH4ACKO3/researchflow.git
cd researchflow
```

2. **Install using uv** :

```bash
uv sync
```

3. **Install development dependencies** (for testing):

```bash
uv sync --group dev
```

#### Option 2: For Existing Projects (Using degit from clean branch)

If you already have a project and want to add ResearchFlow components:

1. **Install degit** (if not already installed):

```bash
npm install -g degit
# or
pnpm install -g degit
# or
yarn global add degit
```

2. **Pull files from clean branch** (overwrites corresponding local files):

```bash
# Pull all files from clean branch
degit CH4ACKO3/researchflow#clean

# Or pull specific directories only
degit CH4ACKO3/researchflow#clean/src/pipelines src/pipelines
degit CH4ACKO3/researchflow#clean/scripts scripts
```

3. **Install dependencies**:

```bash
# Add required dependencies to your pyproject.toml or requirements.txt
uv add nvidia-ml-py h5py

# Or install with pip
pip install nvidia-ml-py h5py
```

4. **Optional: Install development dependencies**:

```bash
uv add --group dev pytest
```

### Dependencies

**Core**:
- `nvidia-ml-py` (>=13.580.82): NVIDIA GPU monitoring
- `h5py`: Data handling

**Development**:
- `pytest` (>=9.0.2): Testing framework

---

## Testing

### Run All Tests

```bash
# Run all tests
uv run pytest

# Run with verbose output
uv run pytest -v

# Run specific test file
uv run pytest tests/pipelines/utils/test_storage.py

# Run concurrent tests
uv run pytest tests/pipelines/utils/test_storage_concurrent.py

# Run scheduler tests (requires GPU)
uv run pytest tests/scripts/test_async_train.py -v -s
```

### Test Coverage Summary

| Component | Test Files | Test Cases | Coverage Areas |
|-----------|------------|------------|----------------|
| **MetadataStorage** | 2 | **52** | Unit tests (42), Concurrency tests (10) |
| **Async Scheduler** | 1 | **2** | Integration tests with multi-GPU validation |
| **Total** | **3** | **54** | Comprehensive unit, integration, and stress tests |

### Detailed Test Coverage

#### MetadataStorage Testing

**Unit Tests** (`tests/pipelines/utils/test_storage.py`): **42 test cases**

- **Initialization**: Storage directory and index creation
- **Entry Creation** (7 tests): Basic creation, custom UUID, duplicate UUID handling, with metadata/extra_info/attachments
- **Entry Updates** (6 tests): Metadata updates, extra_info updates, attachment updates, error handling
- **Entry Queries** (6 tests): UUID queries, metadata partial/exact matching, nested metadata, list queries
- **Entry Deletion** (4 tests): UUID-based deletion, metadata-based deletion, attachment cleanup
- **File Management** (3 tests): Orphaned file cleanup, directory attachments, absolute paths
- **Concurrency** (2 tests): Lock mechanism, nested locking
- **Edge Cases** (5 tests): None values, list values, index persistence, directory auto-creation
- **Integration** (1 test): Multiple operations sequence

**Concurrent Tests** (`tests/pipelines/utils/test_storage_concurrent.py`): **10 test cases**

- **Concurrent Creation**: 10 threads × 10 entries each (100 total)
- **Concurrent Reads**: 10 threads × 20 reads each (200 total)
- **Concurrent Updates**: 5 threads × 10 updates each (50 total)
- **Concurrent Deletes**: 3 threads × 5 deletes each (15 total)
- **Mixed Operations**: 8 threads × 20 mixed ops (160 operations)
- **Create + Read**: 5 creator + 5 reader threads running simultaneously
- **Nested Locking**: 5 threads with nested lock operations
- **Concurrent Cleanup**: 3 threads running cleanup simultaneously
- **Stress Test**: 20 threads × 50 operations (1000 operations)
- **Data Consistency**: Verification of no duplicate UUIDs and valid JSON after all operations

**Total: 52 comprehensive test cases** ensuring reliability under both single-threaded and high-concurrency scenarios.

#### Async GPU Scheduler Testing

**Integration Tests** (`tests/scripts/test_async_train.py`): **2 comprehensive test cases**

1. **Basic Functionality Test**
   - Scenario: 60 tasks with 5-10 second durations on 8 GPUs
   - Expected sequential time: ~450 seconds
   - Expected parallel time: ~56 seconds (8× speedup)
   - Validates: Task completion, log generation, status updates, parallel execution efficiency

2. **Multiple Job Files Test**
   - Scenario: 3 job files × 20 tasks each on 8 GPUs
   - Validates: Multi-file processing, task distribution, completion tracking across files
   - Ensures: All files processed, tasks properly distributed, no conflicts

**Test Coverage Includes**:
- Task state transitions (waiting → running → finished/terminated)
- Log file generation and parsing
- Job file status synchronization
- Multi-GPU parallel execution
- Graceful termination and cleanup
- Task duration and timing accuracy
- Performance ratio verification (parallel vs sequential)

**Total: 2 integration tests** with extensive validation of scheduling efficiency, state management, and multi-GPU coordination.

### Continuous Integration

The test suite includes:
- **Unit tests**: Fast, isolated tests for individual components
- **Integration tests**: End-to-end workflow validation
- **Concurrency tests**: Thread-safety and race condition detection
- **Stress tests**: High-load scenarios (1000+ concurrent operations)

---

## Project Structure

```
researchflow/
├── src/
│   └── pipelines/
│       └── utils/
│           └── storage.py          # MetadataStorage implementation
├── scripts/
│   ├── async_train.py              # GPU scheduler
│   └── test_gpu_work.py            # GPU test utility
├── tests/
│   ├── pipelines/utils/
│   │   ├── test_storage.py         # Storage unit tests (42 cases)
│   │   └── test_storage_concurrent.py  # Concurrency tests (10 cases)
│   └── scripts/
│       └── test_async_train.py     # Scheduler integration tests (2 cases)
├── pyproject.toml                  # Project configuration
└── README.md                       # This file
```

---

## License

This project is licensed under the terms specified in the LICENSE file.

---
